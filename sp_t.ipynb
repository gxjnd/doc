{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sp_t",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJM5ytb668/7UBM8JtksnM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limingzxf/doc/blob/master/sp_t.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slj5iviwnn9J",
        "outputId": "1d47db99-c23b-4981-f61a-24dbdd47469d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jitzgGIzoJJG",
        "outputId": "25fc8fb0-426e-4eb6-ed91-ac17a943366a"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rew9Zs9TG6Wv"
      },
      "source": [
        " \n",
        "import aiohttp\n",
        "import asyncio\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urljoin\n",
        "import re\n",
        "import multiprocessing as mp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wC6rDO7G85r"
      },
      "source": [
        "!pip install aiohttp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug1RLLvckYGj"
      },
      "source": [
        "!touch /content/drive/My\\ Drive/url_id.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufIZ7AbCkqoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccefdfde-c5fc-4884-efb5-00d639583072"
      },
      "source": [
        " !ls /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " aidawn.gscript       data.txt\t\t  user_2.txt\t  无标题文档.gdoc\n",
            " All5en_num.zip       dawnbmbot.gscript   user.txt\n",
            " codechange.gscript   dtbot.gscript\t  view_tg_2.txt\n",
            "'Colab Notebooks'     url_id.txt\t  view_tg.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pANAAIkQlh3F"
      },
      "source": [
        " \n",
        "with open('/content/drive/My Drive/url_id.txt', 'a') as f:\n",
        "  f.write('0\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha-bkT-_kxVL"
      },
      "source": [
        " !cat /content/drive/My\\ Drive/url_id.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN6HKTf-kFRG"
      },
      "source": [
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        "import aiohttp\n",
        "import asyncio\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urljoin\n",
        "import re\n",
        " \n",
        "import os\n",
        "import concurrent\n",
        " \n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        " \n",
        " \n",
        "base_url = \"https://t.me/\"\n",
        "# base_url = \"http://127.0.0.1:4000/\"\n",
        " \n",
        "# DON'T OVER CRAWL THE WEBSITE OR YOU MAY NEVER VISIT AGAIN\n",
        "if base_url != \"http://127.0.0.1:4000/\":\n",
        "  restricted_crawl = True\n",
        "else:\n",
        "  restricted_crawl = False\n",
        " \n",
        "seen = set()\n",
        "unseen = set([base_url])\n",
        " \n",
        "#print(unseen)\n",
        "def parse(html):\n",
        "  soup = BeautifulSoup(html, 'lxml')\n",
        "  #print(html)\n",
        "  urls = soup.find_all('a', {\"href\": re.compile('^/.+?/$')})\n",
        "  print(urls)\n",
        "  title = soup.find('h1').get_text().strip()\n",
        "  page_urls = set([urljoin(base_url, url['href']) for url in urls])\n",
        "  url = soup.find('meta', {'property': \"og:url\"})['content']\n",
        "  return title, page_urls, url\n",
        "  \n",
        "def parse_1(html):\n",
        "  pre = '/content/drive/My Drive/'\n",
        "  usr_t = pre+\"user.txt\"\n",
        "  view_tg_t =   pre+\"view_tg.txt\"\n",
        "  \n",
        "  usr_t_2 = pre+ \"user_2.txt\"\n",
        "  view_tg_t_2 =  pre+ \"view_tg_2.txt\"\n",
        "  \n",
        "  soup = BeautifulSoup(html, 'lxml')\n",
        "  #print(html)\n",
        "  contact = soup.find_all('title')\n",
        "  if (len(contact)==0):\n",
        "    contact=''\n",
        "  elif(len(contact)!=0):\n",
        "    \n",
        "    contact = contact[0].get_text().split('@')\n",
        "    if(len(contact)<2):\n",
        "      contact=''\n",
        "    else:\n",
        "      contact=contact[1]                    \n",
        "#  time.sleep(6)\n",
        " \n",
        "  \n",
        " \n",
        "  \n",
        "  urls_img = soup.find_all('img')\n",
        "  if (len(urls_img)==0):\n",
        "    urls_img=''\n",
        "  elif(len(urls_img)!=0):\n",
        "    urls_img = urls_img[0]['src']                     \n",
        "#  time.sleep(6)\n",
        " \n",
        " \n",
        "  title_b = soup.find_all(\"div\", {\"class\": \"tgme_page_title\"})\n",
        "  if (len(title_b)==0):\n",
        "    title_b=''\n",
        "   # print(html)\n",
        "  elif(len(title_b)!=0):\n",
        "    title_b = title_b[0]('span')[0].get_text()                \n",
        " \n",
        "  \n",
        "  \n",
        "  extra = soup.find_all(\"div\", {\"class\": \"tgme_page_extra\"})\n",
        "  if (len(extra)==0):\n",
        "    extra=''\n",
        "   # print(html)\n",
        "  elif(len(extra)!=0):\n",
        "    extra = extra[0].get_text().strip()\n",
        " \n",
        "                             \n",
        " \n",
        " \n",
        "  description = soup.find_all(\"div\", {\"class\": \"tgme_page_description\"})\n",
        "  if (len(description)==0):\n",
        "    description=''\n",
        "  elif(len(description)!=0):\n",
        "    description = description[0].get_text().strip()                           \n",
        "  \n",
        "  \n",
        "  \n",
        "  send_message = soup.find_all(\"a\", {\"class\": \"tgme_action_button_new\"})\n",
        "  if (len(send_message)==0):\n",
        "    send_message=''\n",
        "   # print(html)\n",
        "  elif(len(send_message)!=0):\n",
        "    send_message = send_message[0].get_text().strip()\n",
        " \n",
        " \n",
        "  send_message_2 = soup.find_all(\"a\", {\"class\": \"tgme_action_button\"})\n",
        "  if (len(send_message_2)==0):\n",
        "    send_message_2=''\n",
        "   # print(html)\n",
        "  elif(len(send_message_2)!=0):\n",
        "    send_message_2 = send_message_2[0].get_text().strip()\n",
        " \n",
        " \n",
        "  \n",
        "  button_label = soup.find_all(\"span\", {\"class\": \"tgme_action_button_label\"})\n",
        "  if (len(button_label)==0):\n",
        "    button_label=''\n",
        "  elif(len(button_label)!=0):\n",
        "    button_label = button_label[0].get_text().strip()                           \n",
        "  \n",
        "  if(send_message==\"Send Message\"):\n",
        "    txt = f\"@{contact} {send_message} {title_b} {extra} {description} {urls_img}  \"           \n",
        " \n",
        "    with open(usr_t, 'a') as f:\n",
        "      f.write(f\"{txt}\\n\")\n",
        "    \n",
        "  elif(send_message==\"View in Telegram\"):\n",
        "    txt = f\"@{contact} {send_message} {title_b} {extra} {description} {urls_img}  \"           \n",
        "    print(txt)\n",
        "    with open(view_tg_t, 'a') as f:\n",
        "      f.write(f\"{txt}\\n\")\n",
        "    \n",
        "  else:\n",
        "    #print(html)\n",
        "    if(send_message_2==\"View in Telegram\"):\n",
        "      txt = f\"@{contact} {send_message} {title_b} {extra} {description} {urls_img}  \"           \n",
        "      print(txt)\n",
        "      with open(view_tg_t_2, 'a') as f:\n",
        "        f.write(f\"{txt}\\n\")\n",
        "      \n",
        "    elif(send_message_2==\"Send Message\"):\n",
        "      txt = f\"@{contact} {send_message} {title_b} {extra} {description} {urls_img}  \"           \n",
        "  \n",
        "      with open(usr_t_2, 'a') as f:\n",
        "        f.write(f\"{txt}\\n\")\n",
        "      \n",
        "    else:\n",
        "      pass\n",
        "      #print(html)\n",
        "      print(f\"@{contact} {urls_img} {title_b} {extra} {description} {send_message}  \")\n",
        "  '''\n",
        "  print(urls_img)\n",
        "  print(title_b)\n",
        "  print(extra)\n",
        "  print(description)\n",
        "  print(send_message)\n",
        "  '''\n",
        " # return send_message,extra,title_b\n",
        "  \n",
        "  #for image in urls_img:\n",
        "   # print(image['src'])\n",
        "  #print(urls_img)\n",
        "  #print(len(urls_img))\n",
        "  '''\n",
        "  title = soup.find('h1').get_text().strip()\n",
        "  page_urls = set([urljoin(base_url, url['href']) for url in urls])\n",
        "  url = soup.find('meta', {'property': \"og:url\"})['content']\n",
        "  return title, page_urls, url\n",
        "  '''\n",
        " \n",
        " \n",
        "def read_l(begin,line):\n",
        "  pre='/content/drive/My Drive/'\n",
        "  data_t =  pre+\"data.txt\"\n",
        "  t = []\n",
        "  with open(data_t, \"rb\") as file:\n",
        "    if(begin==1):\n",
        "      file.seek(0, os.SEEK_SET)\n",
        "      for k in range(line):\n",
        "        read_result = file.read(5)\n",
        "        file.seek(1, os.SEEK_CUR)\n",
        "        t.append(read_result.decode())\n",
        "       \n",
        "    else:\n",
        "      start = (begin -1)*6\n",
        "      file.seek(start, os.SEEK_SET)\n",
        "      for k in range(line):\n",
        "        read_result = file.read(5)\n",
        "        file.seek(1, os.SEEK_CUR)\n",
        "        t.append(read_result.decode())\n",
        "  return t\n",
        " \n",
        "  \n",
        "  \n",
        "  \n",
        "async def crawl(url, session):\n",
        "  r = await session.get(url)\n",
        "  html = await r.text()\n",
        "  await asyncio.sleep(0.1)        # slightly delay for downloading\n",
        "  \n",
        "  return html\n",
        "  \n",
        " \n",
        "async def main(loop):\n",
        "  pre='/content/drive/My Drive/'\n",
        "  url_id_t =  pre+\"url_id.txt\"\n",
        "  while 1:\n",
        "      \n",
        "    with open(url_id_t, \"rb\") as file:\n",
        "      file.seek(-2, os.SEEK_END)\n",
        "      while file.read(1) != b'\\n':\n",
        "        if (file.seek(-2, os.SEEK_CUR) == 0):\n",
        "          break\n",
        "      tail_c = file.readline().decode().strip()\n",
        "    print(tail_c)\n",
        "    \n",
        "    \n",
        "    start=int(tail_c)+ 1\n",
        "    number_of_lines = 1000\n",
        "    #  pool = mp.Pool(2)               # slightly affected\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "      count = 1\n",
        "      d_t=[]\n",
        "      \n",
        "      d_t = read_l(start,number_of_lines)\n",
        "      #print(d_t) \n",
        "      \n",
        "  \n",
        "      len_d_t = len(d_t)\n",
        "      tasks = []\n",
        "      for i in range(0,len_d_t):\n",
        "        end_url =d_t[i]\n",
        "        url = f\"{base_url}{end_url}\"\n",
        "        task = loop.create_task(crawl(url, session))\n",
        "        print(\"finish\")\n",
        "        tasks.append(task)\n",
        "      finished = await asyncio.gather(*tasks)\n",
        "      with open(url_id_t, 'a') as f:\n",
        "        f.write(f\"{int(tail_c)+number_of_lines}\\n\")\n",
        "  \n",
        "    #  for i in range(len(finished)):\n",
        "       # url_list = finished[i]\n",
        "       # print(url_list)\n",
        "      with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        ##print(len(finished))\n",
        "        results = executor.map(parse_1, finished)\n",
        "       # for result in results:\n",
        "        #  print()\n",
        "      #print(len(finished))\n",
        "      #print()\n",
        "      \n",
        "      #with open('wb.txt', 'w') as f:\n",
        "       # f.write(f\"{finished[2]}\")\n",
        "      '''\n",
        "      for i in range(len(finished)):\n",
        "        parse_jobs = pool.apply_async(func=parse_1, args=(finished[i],))\n",
        "      results = [j.get() for j in parse_jobs]\n",
        "      print(results)\n",
        "      '''\n",
        "      #parse_1(finished[1])\n",
        "      #htmls = [f.result() for f in finished]\n",
        "      #parse_jobs = [pool.apply_async(parse_1, args=(html,)) for html in htmls]\n",
        "     \n",
        "        \n",
        "        \n",
        "  \n",
        "  '''\n",
        "    htmls = [f.result() for f in finished]\n",
        "    \n",
        "    results = [j.get() for j in parse_jobs]\n",
        "    \n",
        "    seen.update(unseen)\n",
        "    unseen.clear()\n",
        "    for title, page_urls, url in results:\n",
        "        print(count, title, url)\n",
        "        unseen.update(page_urls - seen)\n",
        "        count += 1\n",
        "  \n",
        "  '''\n",
        " \n",
        " \n",
        "            \n",
        "if __name__ == \"__main__\":\n",
        "  t1 = time.time()\n",
        "  loop = asyncio.get_event_loop()\n",
        "  loop.run_until_complete(main(loop))\n",
        "  loop.close()\n",
        "  print(\"Async total time: \", time.time() - t1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjXxVJbnl9WF"
      },
      "source": [
        " \n",
        "import asyncio\n",
        "import time\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        " \n",
        "i = 1\n",
        " \n",
        " \n",
        "async def get():\n",
        "  print(\"get\")\n",
        "  await asyncio.sleep(1)\n",
        "  print(\"after 1s\")\n",
        "  await asyncio.sleep(2)\n",
        "  print(\"after 2s\")\n",
        " \n",
        "async def main(loop):\n",
        "  global i\n",
        "  print(i)\n",
        "  tasks =[]\n",
        "  for _ in range(10):\n",
        "    #task = loop.create_task(get())\n",
        "    print(\"after t\")  \n",
        "    #task = get()\n",
        "    task = loop.create_task(get())\n",
        "    print(\"finish\")\n",
        "    tasks.append(task)\n",
        "    \n",
        "    \n",
        "  await asyncio.gather(*tasks)\n",
        "  await asyncio.sleep(0)\n",
        "    \n",
        "  i=i+1\n",
        "  print(\"end\")\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  \n",
        "  s = time.perf_counter()\n",
        "  \n",
        "  loop = asyncio.get_event_loop()\n",
        "  loop.run_until_complete(main(loop))\n",
        "  elapsed = time.perf_counter() - s\n",
        "  print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}